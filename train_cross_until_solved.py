from dataclasses import dataclass
from model_cross import ConvLSTMClassifier, ConvLSTM
from cube_bindings import Cube
import torch
import torch.nn as nn
from torch import Tensor
from typing import Tuple, List, Callable


"""
train_cross_until_solved.py is a modification of the original train_cross.py file. The difference is in how training happens within 
each epoch. 

In the original train_cross.py file, a set of solutions for the batch is generated at the start of each epoch that is iterated through
while the model predicts the correct move at each iteration. After the length of the longest solution has elapsed, the epoch is over, 
regardless of whether the model has solved the cube or not. 

The changes made in this file are to keep training in the current epoch until the model has solved the cube. This requires making the 
train function work a bit 
"""


@dataclass
class TrainConfig:
    scramble_len :int
    epochs :int
    val_num_batches :int
    batch_size :int
    lr :float
    device: torch.device
    optimizer_factory: Callable[[nn.Module], torch.optim.Optimizer]
    criterion: nn.Module = nn.CrossEntropyLoss()
    num_classes :int = 13
    max_time_steps :int = 10

class Trainer:
    def __init__(t, cube : Cube, config :TrainConfig, model :nn.Module) -> None:

        #NOTE: I am using t instead of self

        #unpack train config
        t.epochs = config.epochs
        t.batch_size = config.batch_size
        t.lr = config.lr
        t.device = config.device
        t.optimizer = config.optimizer_factory(model)
        t.criterion = config.criterion
        t.scramble_len = config.scramble_len
        t.num_classes = config.num_classes
        t.max_time_steps = config.max_time_steps 

        # python bindings to the cube rust program
        t.cube = cube 

        # The model will output a prediction between 12 classes, one for each possible move
        # This list will be used to convert the output of the model to a string representation of the move
        t.moves = ["U", "U'", "D", "D'", "L", "L'", "R", "R'", "F", "F'", "B", "B'", ""]

        # model
        t.model = model
        t.model.to(t.device)

        # train stats
        t.train_loss = [ 0 for _ in range(t.epochs) ]

        # The train accuracy is calculated by correct-moves / total-moves per epoch
        t.train_acc = [ 0 for _ in range(t.epochs) ]  


    def train(t) -> None:
        """
        The train() function works differently than a typical pytorch training loop. This is because of the semi-reinforced nature of 
        training. 

        The loss is predicated on the model predicting the next correct move in the solution as defined by the solution generated by the 
        cross solver function. This means that at the start of each epoch, a new batch of cube_states, scrambles, and cross solutions will
        be generated to serve as the training data of the model. 
        """

        print("Training Model...")

        for epoch in range(t.epochs):

            # generate and normalize a batch of training data 
            batch : Tuple[Tensor, list[str]] = t.get_batch()
            cube_states, all_moves_made = t.nomalize(batch[0]), batch[1] 

            t.train_on_batch(cube_states, all_moves_made)
  


    def train_on_batch(t, cube_states :Tensor, all_moves_made :list[str]) -> None:

        solved : bool = False

        while not solved:

            # generate a new cross solution for the moves made so far 
            one_hot_solutions : Tensor = t.next_correct_move(all_moves_made)

            print(f"one_hot_solutions.shape: {one_hot_solutions.shape}")

            # forward pass and inference
            outputs : Tensor = t.model(cube_states.to(t.device)) # likelihood distr over possible moves
            pred : Tensor = torch.argmax(outputs, dim=1) # indices of the most likely move

            print(f"outputs.shape: {outputs.shape}, pred.shape: {pred.shape}")

            # update the state of the cube and all_moves_made strings with the predicted move
            updated_states : Tuple[Tensor, list[str]] = t.update_cube_state_series(pred, all_moves_made, cube_states)
            cube_states, all_moves_made = updated_states[0], updated_states[1]

            print(f"cube_states.shape: {cube_states.shape}, len(all_moves_made): {len(all_moves_made)}")
             
            # calculate loss
            loss = t.criterion(outputs, one_hot_solutions)
            loss.backward()
            t.optimizer.step()
            t.optimizer.zero_grad()


    def update_cube_state_series(t, pred : Tensor, moves_made_so_far : list[str], cube_states_so_far : Tensor) -> Tuple[Tensor, list[str]]:
        ''' This function updates the cube state series by applying the move predicted by the model to the cube state series. This is done
        by first joining the move to each string in all_moves_made, then using the cube binding to apply all the moves to the cube state series.
        The tensor that is returned is normalized and concatenated to the existing cube state series. Earlier time steps are removed if the
        series is longer than max_time_steps.'''

        # update all_moves_made to include the models pred
        all_moves_made : list[str] = [ " ".join([moves_made_so_far[i], t.moves[pred[i]]]) for i in range(t.batch_size) ]
        next_cube_state : Tensor = t.cube.apply_moves(all_moves_made).unsqueeze(1).unsqueeze(1) # <-- unsqueeze channels and time series dim

        # concat the normalized next_cube_state to the cube_states along the time-step dimmension
        cube_states : Tensor = torch.cat((cube_states_so_far, t.nomalize(next_cube_state)), dim=1)

        # keep times steps under max_time_steps
        if cube_states.shape[1] > t.max_time_steps:
            cube_states = t.remove_initialmost_time_step(cube_states)

        return cube_states, all_moves_made

    def remove_initialmost_time_step(t, cube_states :Tensor) -> Tensor:
        ''' This function removes the initialmost time step in the series in to implement a mechanism 
        for the time series doesn't grow so large it is unmanaganle in terms of compute and memory.'''
        return cube_states[:, 1:, :, :, :, :]


    def next_correct_move(t, scrambles :list[str]) -> Tensor:
        '''
        This funciton is used to generate the true solutions for the scrambles generated by the cube bindings.
        The true solutions are generated by the cross solver program within the cube bindings.

        The function will return a list of one hot encoded solutions tensors as well as the max length of the 
        solutions. 
        
        Each Tensor in the list contains all correct moves for the batch for that indice's time step.
        Meaning the shape will be (batch_size, 13) <=== where 13 is a one hot encoded vector of the correct move
        '''

        # each str is an individual move in the solution
        solutions : list[list[str]] = [t.cube.solve_cross(scramble).split() for scramble in scrambles] 
        solutions = [ [move.replace("''", "'") for move in solution] for solution in solutions ]  # Remove any duplicate apostrophies in solutions

        # get the next correct move for each scramble in the batch (0'th move)
        correct_move_indices : list[int] = [ t.moves.index(solutions[b][0]) for b in range(t.batch_size) ]
        one_hot_tensor_target = torch.zeros((t.batch_size, t.num_classes))

        # Fill in the ones based on the integers
        for b in range(t.batch_size):
            # place a 1 in the correct move idx
            one_hot_tensor_target[b, correct_move_indices[b]] = 1

        return one_hot_tensor_target

    def nomalize(t, data : Tensor) -> Tensor:
        ''' Because the cube is integer encoded form 1-6, and we want to normalize the data
        between 0 and 1, AND we don't want the data to be too sparse because of all the 
        0 elements, we add 1 to all elements and divide by 7. This will give us a range
        between 0 and 1, and the data will be dense. '''
        return (data + 1) / 7 # <--- broadcasting


    def get_batch(t) -> Tuple[Tensor, list[str]]:
        '''gen_batch() calls on the Cube python bindings to generate a batch of scrambled cube states and their corresponding scrambles.

        All of the additional logic is used to check that all of the scrambles generated by the solved are solvable by the cross solver 
        program within Cube. If any of the scrambles are not solveable, then they are removed from the batch and replaced with new ones.

        The function returns a tuple of the cube states and their corresponding scrambles. 
        NOTE: The cube state is a torch Tensor of shape (batch_size, time_steps, channels, depths, width, height)'''

        all_solved : bool = False
        while not all_solved:

            solvable : Tuple[Tensor, list[str], bool] = t.generate_solvable()
            cube_states, scrambles, full_batch = solvable[0], solvable[1], solvable[2]

            if not full_batch:

                new_solvable : Tuple[Tensor, list[str], bool] = t.generate_solvable()
                new_cube_states, new_scrambles = new_solvable[0], new_solvable[1]
                    
                # concatenate the new solvable cube states and scrambles to the old ones
                elements_needed : int = t.batch_size - cube_states.shape[0]

                assert elements_needed > 0, "elements_needed must be greater than 0"
                assert len(new_scrambles) >= elements_needed and len(new_cube_states) >= elements_needed, "not enough new scrambles or cube states"
                for i in range(elements_needed):

                    cube_states = torch.cat((cube_states, new_cube_states[i].unsqueeze(0)))
                    scrambles.append(new_scrambles[i]) 

                # check if the scrambles are solvable by the cross solver program
                all_solved = all([ " ".join([scrambles[i], t.cube.solve_cross(scramble)]) for (i, scramble) in enumerate(scrambles)])

        # unsqueeze is used to add the channels and time series dimmensions               
        return cube_states.unsqueeze(1).unsqueeze(1), scrambles



            
    def generate_solvable(t) -> Tuple[Tensor, list[str], bool]:
        """ This function is used to generate a batch of cube states and scrambles that are solvable by the cross solver program within the
        cube bindings. Also returned is a boolean, full_batch, of whether the num elements in the batch is equal to the batch size. """

        # This lambda is used to check if the scramble is solvable by the cross solver program
        check_solution : Callable[[str, str], bool] = lambda scramble, solution : t.cube.is_cross_solved(" ".join([scramble, solution]))

        # generate a batch of cube states and scrambles
        data : Tuple[Tensor, list[str]] = t.cube.generate_data(t.batch_size, t.scramble_len)
        cube_states, scrambles = data[0], data[1]

        # check if the scrambles are solvable by the cross solver program
        cross_solutions : list[str] = [ t.cube.solve_cross(scramble) for scramble in scrambles ]
        is_cross_solved : list[bool] = [ check_solution(scrambles[i], solution) for (i, solution) in enumerate(cross_solutions)]

        # full_batch is used to check if the number of solvable cube states matches the batch size
        full_batch : bool = all(is_cross_solved)

        # Use boolean indexing to filter cube_states and scrambles to only include the ones that are solvable
        solvable_cube_states : Tensor = cube_states[torch.tensor(is_cross_solved)]
        solvable_scrambles : list[str] = [ scrambles[i] for (i, is_true) in enumerate(is_cross_solved) if is_true ]

        return solvable_cube_states, solvable_scrambles, full_batch
            




if __name__ == "__main__":

    cube = Cube()

    config = TrainConfig(
        scramble_len=40,
        epochs=1,
        val_num_batches=10,
        batch_size=32,
        lr=0.001,
        device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
        optimizer_factory=lambda model: torch.optim.Adam(model.parameters(), lr=0.001),
        num_classes=13,
        max_time_steps = 10
    )

    print(f"using device: {config.device}")

   # Initialize the ConvLSTM
    conv_lstm = ConvLSTM(input_channels=1, hidden_channels=[8, 16, 32, 32, 64], kernel_size=3)

    num_output_features = 64 * 5 * 5 * 5  # Replace with the correct size

    # Initialize ConvLSTMClassifier
    classifier = ConvLSTMClassifier(conv_lstm, num_output_features, num_classes=13)


    trainer = Trainer(cube, config, classifier)

    trainer.train()
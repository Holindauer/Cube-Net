from dataclasses import dataclass
from early_stopping import EarlyStopping
from cube_bindings import Cube
import torch
import torch.nn as nn
from torch import Tensor

@dataclass
class TrainConfig:
    scramble_len :int
    epochs :int
    val_num_batches :int
    batch_size :int
    lr :float
    device: torch.device
    optimizer :torch.optim.Optimizer
    early_stopping :EarlyStopping
    criterion: nn.Module = nn.CrossEntropyLoss()
    num_classes :int = 13

class Trainer:
    def __init__(self, cube : Cube, config :TrainConfig, model :nn.Module) -> None:

        #unpack train config
        self.epochs = config.epochs
        self.batch_size = config.batch_size
        self.lr = config.lr
        self.device = config.device
        self.optimizer = config.optimizer(model.parameters(), lr=config.lr)
        self.criterion = config.criterion
        self.scramble_len = config.scramble_len
        self.num_classes = config.num_classes

        # setup early stopping
        self.early_stopping = config.early_stopping

        # python bindings to the cube rust program
        self.cube = cube 

        # The model will output a prediction between 12 classes, one for each possible move
        # This list will be used to convert the output of the model to a string representation of the move
        self.moves = ["U", "U'", "D", "D'", "L", "L'", "R", "R'", "F", "F'", "B", "B'", ""]

        # model
        self.model = model
        self.model.to(self.device)

        # train stats
        self.train_loss = [ 0 for _ in range(self.epochs) ]

        # The train accuracy is calculated by correct-moves / total-moves per epoch
        self.train_acc = [ 0 for _ in range(self.epochs) ]  


    def train(self) -> None:
        """
        The train() function works differently than a typical pytorch training loop. This is because of the semi-reinforced nature of 
        training. 

        The loss is predicated on the model predicting the next correct move in the solution as defined by the solution generated by the 
        cross solver function. This means that at the start of each epoch, a new batch of cube_states, scrambles, and cross solutions will
        be generated to serve as the training data of the model. 
        """

        print("Training Model...")

        for i in range(self.epochs):
            
            # generate a batch of training data
            cube_states, scrambles = self.get_batch()
            cube_states = self.nomalize(cube_states)

            # generate the true solutions for the scrambles
            one_hot_true_solutions, max_solution_len = self.generate_solutions(scrambles)

            # iterate through the moves of true solutions
            for move in range(max_solution_len):

                print(f" {move} ", end="")

                cube_states = cube_states.to(self.device)

                outputs = self.model(cube_states) # probabilitiy distribution over the 12 possible moves
                pred = torch.argmax(outputs, dim=1) # index of the most likely move

                # update scrambles to include the models pred
                moves = [ self.moves[pred[i]] for i in range(self.batch_size) ] # convert move pred to string representation
                all_moves_made = [ " ".join([scrambles[i], moves[i]]) for i in range(self.batch_size) ]
                next_cube_state = self.cube.apply_moves(all_moves_made).unsqueeze(1).unsqueeze(1) # <-- unsqueeze to add channels and time series dimmensions

                # normalize the next cube state
                next_cube_state = self.nomalize(next_cube_state)
                
                # concat the next cube state to the cube states along the time-step dimmension
                cube_states = torch.cat((cube_states, next_cube_state), dim=1)
                
                # calculate loss
                loss = self.criterion(outputs, one_hot_true_solutions[move])
                loss.backward()
                self.optimizer.step()
                self.optimizer.zero_grad()

                # incrememnent train loss (prior to averaging)
                self.train_loss[i] += loss.item()

                # take the argmax of the one hot encoded true solutions to get the targets
                target = torch.argmax(one_hot_true_solutions[move], dim=1)

                self.train_acc[i] += torch.sum(pred == target).item()  # <--- incrememnt acc

            # average the epoch loss over the number of moves in the solution
            self.train_loss[i] /= max_solution_len
            self.train_acc[i] /= (max_solution_len * self.batch_size)

            print(f'\nepoch: {i} | train loss: {self.train_loss[i]} | train acc: {self.train_acc[i]}')

    def remove_initialmost_time_step(self, cube_states :Tensor) -> Tensor:
        '''
        This function removes the initialmost time step in the series in to implement a mechanism 
        for the time series doesn't grow so large it is unmanaganle in terms of compute and memory.
        '''

        # remove the initialmost time step
        return cube_states[:, 1:, :, :, :, :]


    def generate_solutions(self, scrambles :list[str]) -> (list[list[str]], int):
        '''
        This funciton is used to generate the true solutions for the scrambles generated by the cube bindings.
        The true solutions are generated by the cross solver program within the cube bindings.

        The function will return a list of one hot encoded solutions tensors as well as the max length of the 
        solutions. 
        
        Each Tensor in the list contains all correct moves for the batch for that indice's time step.
        Meaning the shape will be (batch_size, 13) <=== where 13 is a one hot encoded vector of the correct move
        '''

        # This is a list of list of strings. (each str is an individual move)
        true_solutions = [self.cube.solve_cross(scramble).split() for scramble in scrambles] 

        # Remove any duplicate apostrophies
        true_solutions = [ [move.replace("''", "'") for move in solution] for solution in true_solutions ]

        # append empty strings to the any actual solutions that are less than the longest actual solution list until they match that length
        max_solution_len = max([len(solution) for solution in true_solutions])
        for j in range(self.batch_size):
            while len(true_solutions[j]) < max_solution_len:
                true_solutions[j].append("")

        # one hot encode the true solutions
        one_hot_solutions = []
        for move in range(max_solution_len):
            correct_move_indices = [ self.moves.index(true_solutions[b][move]) for b in range(self.batch_size) ]
            one_hot_tensor_target = torch.zeros((self.batch_size, self.num_classes))

            # Fill in the ones based on the integers
            for b in range(self.batch_size):
                # place a 1 in the correct move idx
                one_hot_tensor_target[b, correct_move_indices[b]] = 1

            one_hot_solutions.append(one_hot_tensor_target)

        return one_hot_solutions, max_solution_len

    def nomalize(self, data : Tensor) -> Tensor:
        '''
        Because the cube is integer encoded form 1-6, and we want to normalize the data
        between 0 and 1, AND we don't want the data to be too sparse because of all the 
        0 elements, we add 1 to all elements and divide by 7. This will give us a range
        between 0 and 1, and the data will be dense.
        '''
        # braodcast addition by 1 and div by 7 to get values between 0 and 1
        return (data + 1) / 7


    def get_batch(self) -> (Tensor, list[str]):
        '''
        gen_batch() calls on the Cube python bindings to generate a batch of scrambled cube states and their corresponding scrambles.

        All of the additional logic is used to check that all of the scrambles generated by the solved are solvable by the cross solver 
        program within Cube. If any of the scrambles are not solveable, then they are removed from the batch and replaced with new ones.

        The function returns a tuple of the cube states and their corresponding scrambles. 
        NOTE: The cube state is a torch Tensor of shape (batch_size, time_steps, channels, depths, width, height)
        '''

        all_solved = 0

        # This lambda function is used to check if the scramble is solvable by the cross solver program
        check_solution = lambda scramble, solution : self.cube.is_cross_solved(" ".join([scramble, solution]))

        while not all_solved:

            cube_states, scrambles = self.cube.generate_data(self.batch_size, self.scramble_len)

            cross_solutions = [ self.cube.solve_cross(scramble) for scramble in scrambles ]
            is_cross_solved = [ check_solution(scrambles[i], solution) for (i, solution) in enumerate(cross_solutions)]

            # check that all of the scrambles are solvable by the cross solver program
            if not all(is_cross_solved):

                # Use boolean indexing to filter cube_states and scrambles to only include the ones that are solvable
                solvable_cube_states = cube_states[torch.tensor(is_cross_solved)]
                solvable_scrambles  = [ scrambles[i] for (i, is_true) in enumerate(is_cross_solved) if is_true ]

                # ensure that each batch is the correct size and that all of the scrambles are solvable
                while not solvable_cube_states.shape[0] == self.batch_size:

                    # generate new scrambles and cube states
                    cube_states, scrambles = self.cube.generate_data(self.batch_size, self.scramble_len)
                    cross_solutions = [ self.cube.solve_cross(scramble) for scramble in scrambles ]
                    is_cross_solved = [ check_solution(scrambles[i], solution) for (i, solution) in enumerate(cross_solutions)]

                    # Use boolean indexing to filter cube_states and scrambles to only include the ones that are solvable
                    new_solvable_cube_states = cube_states[torch.tensor(is_cross_solved)]
                    new_solvable_scrambles  = [ scrambles[i] for (i, is_true) in enumerate(is_cross_solved) if is_true ]

                    # concatenate the new solvable cube states and scrambles to the old ones
                    for i in range(self.batch_size - solvable_cube_states.shape[0]):
                        solvable_cube_states = torch.cat((solvable_cube_states, new_solvable_cube_states[i].unsqueeze(0)))
                        solvable_scrambles.append(new_solvable_scrambles[i])    

                # unsqueeze is used to add the channels and time series dimmensions
                return solvable_cube_states.unsqueeze(1).unsqueeze(1), solvable_scrambles

            else : # unsqueeze is used to add the channels and time series dimmensions
                return cube_states.unsqueeze(1).unsqueeze(1), scrambles
            






